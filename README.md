# ğŸ“˜ Projeto Airflow CNPJ â€“ Guia de InstalaÃ§Ã£o e Uso

Este repositÃ³rio contÃ©m um pipeline completo de processamento de dados do CNPJ utilizando **Apache Airflow**, **PySpark** e execuÃ§Ã£o dentro do **WSL (Ubuntu)**. O objetivo Ã© fornecer um ambiente simples, reprodutÃ­vel e automatizado para extraÃ§Ã£o, limpeza, transformaÃ§Ã£o e monitoramento dos dados pÃºblicos de empresas brasileiras.

---

## ğŸš€ PrÃ©-requisitos

Antes de iniciar, vocÃª precisa ter instalado no Windows:

* **Windows 10 ou 11** atualizado
* **WSL2 (Windows Subsystem for Linux)**
* **Ubuntu 22.04 ou superior no WSL**
* **Docker Desktop** com integraÃ§Ã£o habilitada para WSL2

---

## ğŸ§ 1. Instalando o WSL + Ubuntu

Abra o PowerShell **como administrador** e execute:

```powershell
wsl --install -d Ubuntu
```

ApÃ³s finalizar, abra o Ubuntu no menu iniciar e configure seu usuÃ¡rio.

Atualize o sistema:

```bash
sudo apt update && sudo apt upgrade -y
```

---

## ğŸ³ 2. Instalando o Docker no WSL

No Windows, instale o **Docker Desktop**:

ğŸ‘‰ [https://www.docker.com/products/docker-desktop/](https://www.docker.com/products/docker-desktop/)

Durante a instalaÃ§Ã£o, habilite:

âœ” "Enable WSL integration"
âœ” "Use WSL 2 instead of Hyper-V"

Depois, no Ubuntu, valide:

```bash
docker --version
docker compose version
```

Se funcionar, estÃ¡ pronto!

---

## ğŸŒ¬ï¸ 3. Instalando o Apache Airflow (via Docker Compose)

Este projeto jÃ¡ inclui toda a estrutura necessÃ¡ria para rodar o Airflow.

No WSL, crie o diretÃ³rio padrÃ£o do Airflow:

```bash
sudo mkdir -p /opt/airflow
sudo chmod -R 777 /opt/airflow
```

---

## ğŸ“ 4. Clonando o repositÃ³rio e organizando os arquivos

Clone o repositÃ³rio dentro do Ubuntu:

```bash
git clone https://github.com/seu_usuario/seu_repositorio.git
cd seu_repositorio
```

Agora copie a estrutura para dentro de `/opt/airflow`:

```bash
cp -R dags data docker-compose.yaml .env /opt/airflow/
```

A estrutura final deve ficar assim:

```
/opt/airflow
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ python_pipeline_cnpj.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ cnpj_abertos/
â”‚       â”œâ”€â”€ bronze/
â”‚       â”œâ”€â”€ silver/
â”‚       â”œâ”€â”€ gold/
â”‚       â””â”€â”€ monitoramento/
â”œâ”€â”€ docker-compose.yaml
â””â”€â”€ .env
```

---

## â–¶ï¸ 5. Subindo o Airflow

Dentro de `/opt/airflow`, execute:

```bash
docker compose up -d
```

Isso irÃ¡ iniciar:

* Scheduler
* Webserver
* Worker
* Redis
* Postgres

A interface do Airflow estarÃ¡ em:

ğŸ‘‰ [http://localhost:8080](http://localhost:8080)

UsuÃ¡rio padrÃ£o:

```
user: airflow
password: airflow
```

---

## ğŸ“ 6. Estrutura do Pipeline

O DAG principal estÃ¡ em:

```
dags/python_pipeline_cnpj.py
```

Ele contÃ©m as seguintes etapas:

1. **extract_bronze** â€“ download e extraÃ§Ã£o dos dados
2. **analisar_qualidade** â€“ mÃ©tricas de qualidade e estatÃ­sticas iniciais
3. **limpar_dados** â€“ tratamento e padronizaÃ§Ã£o
4. **transformar_dados** â€“ enriquecimento e geraÃ§Ã£o do dataset final
5. **gerar_estatisticas** â€“ KPIs e grÃ¡ficos
6. **load_database** â€“ envio para um banco SQLite/Parquet
7. **monitor_pipeline** â€“ coleta de mÃ©tricas de execuÃ§Ã£o

---

## ğŸ“Š 7. Monitoramento e GrÃ¡ficos

O pipeline gera automaticamente:

* Um arquivo CSV com as mÃ©tricas por task
* GrÃ¡ficos de duraÃ§Ã£o
* GrÃ¡ficos de registros processados
* GrÃ¡ficos estatÃ­sticos adicionais

Eles ficam em:

```
/opt/airflow/data/cnpj_abertos/monitoramento/
```

Inclui arquivos como:

* **monitoramento_log.csv**
* **duracao_por_task.png**
* **registros_por_task.png**
* **estatisticas_resumo.json**

---

## ğŸ”„ 8. Reiniciando o Airflow

```bash
docker compose down
```

E subir novamente:

```bash
docker compose up -d
```

---

## ğŸ§¹ 9. Limpando o ambiente

Para excluir tudo (incluindo banco e logs):

```bash
sudo rm -rf /opt/airflow
```

---

## ğŸ¤ ContribuiÃ§Ãµes

Pull requests sÃ£o bem-vindos! Sinta-se Ã  vontade para abrir issues com sugestÃµes ou correÃ§Ãµes.

---

## â­ DÃª uma estrela!

Se este projeto ajudou vocÃª, deixe uma â­ no repositÃ³rio!
